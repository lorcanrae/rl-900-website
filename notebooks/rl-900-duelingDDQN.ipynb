{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67381f35-7f4c-40e6-a37f-04cbaa8f59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Setup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b2bd055-1b32-425e-8473-0ad4b03fdc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:22: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import gym\n",
    "from gym.wrappers.atari_preprocessing import AtariPreprocessing\n",
    "from gym.wrappers.frame_stack import FrameStack\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603b6d38-e28d-4a79-a7cb-e588c125ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environmnent and preprocess\n",
    "def instantiate_environmnent():\n",
    "    env = gym.make(\"SpaceInvadersNoFrameskip-v4\")\n",
    "    env = AtariPreprocessing(env, grayscale_newaxis=False, frame_skip=5)\n",
    "    env = FrameStack(env, 4)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f2dc7b-069b-4583-b453-d07f7669325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 6\n",
    "\n",
    "def create_q_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(84, 84, 4))\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer = layers.Conv2D(32, 8, activation=\"relu\")(inputs)\n",
    "    layer = layers.MaxPool2D(pool_size=(4, 4))(layer)\n",
    "    layer = layers.Conv2D(64, 4, activation=\"relu\")(layer)\n",
    "    layer = layers.MaxPool2D(pool_size=(2, 2))(layer)\n",
    "    layer = layers.Conv2D(64, 3, activation=\"relu\")(layer)\n",
    "\n",
    "    layer = layers.Flatten()(layer)\n",
    "\n",
    "    layer = layers.Dense(512, activation=\"relu\")(layer)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0422886e-9d9c-4e28-b5de-9700e721279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(reward):\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99e4b553-d7ab-438d-aeab-1334c1348f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.scale = tf.keras.layers.Lambda(\n",
    "            lambda x: x/255\n",
    "        )\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=8,\n",
    "            strides=4,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(2.0),\n",
    "            bias_initializer=tf.keras.initializers.Zeros(),\n",
    "            data_format=\"channels_last\",\n",
    "            input_shape=(84,84,4)\n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=4,\n",
    "            strides=2,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(2.0),\n",
    "            bias_initializer=tf.keras.initializers.Zeros(),\n",
    "            data_format=\"channels_last\",\n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(2.0),\n",
    "            bias_initializer=tf.keras.initializers.Zeros(),\n",
    "            data_format=\"channels_last\",\n",
    "        )\n",
    "        self.flatten =tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(\n",
    "            units=512,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(2.0),\n",
    "            bias_initializer=tf.keras.initializers.Zeros(),\n",
    "        )\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        self.A = tf.keras.layers.Dense(num_actions)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, states):\n",
    "        x = self.scale(states)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        Q = V + tf.subtract(A, tf.reduce_mean(A, axis=1, keepdims=True))\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f72388b-9cd3-497a-a40e-80e012126530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "----------\n",
      "Game Number: 1\n",
      "EXPLORATION\n",
      "Score: 225.0\n",
      "Reward: 225.0\n",
      "Timesteps: 493\n",
      "Game Frames Survived: 2491\n",
      "Epsilon: 0.9991125999999745\n",
      "Running Score (last 10 games): 225.0\n",
      "Running Reward (last 10 games): 225.0\n",
      "Explored: 1, Exploited: 0\n",
      "INFO:tensorflow:Assets written to: model3/assets\n",
      "----------\n",
      "----------\n",
      "Game Number: 2\n",
      "EXPLORATION\n",
      "Score: 85.0\n",
      "Reward: 85.0\n",
      "Timesteps: 408\n",
      "Game Frames Survived: 2065\n",
      "Epsilon: 0.9983781999999534\n",
      "Running Score (last 10 games): 155.0\n",
      "Running Reward (last 10 games): 155.0\n",
      "Explored: 2, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 3\n",
      "EXPLORATION\n",
      "Score: 65.0\n",
      "Reward: 65.0\n",
      "Timesteps: 218\n",
      "Game Frames Survived: 1095\n",
      "Epsilon: 0.9979857999999421\n",
      "Running Score (last 10 games): 125.0\n",
      "Running Reward (last 10 games): 125.0\n",
      "Explored: 3, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 4\n",
      "EXPLORATION\n",
      "Score: 120.0\n",
      "Reward: 120.0\n",
      "Timesteps: 410\n",
      "Game Frames Survived: 2061\n",
      "Epsilon: 0.9972477999999209\n",
      "Running Score (last 10 games): 123.75\n",
      "Running Reward (last 10 games): 123.75\n",
      "Explored: 4, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 5\n",
      "EXPLORATION\n",
      "Score: 120.0\n",
      "Reward: 120.0\n",
      "Timesteps: 402\n",
      "Game Frames Survived: 2013\n",
      "Epsilon: 0.9965241999999\n",
      "Running Score (last 10 games): 123.0\n",
      "Running Reward (last 10 games): 123.0\n",
      "Explored: 5, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 6\n",
      "EXPLORATION\n",
      "Score: 60.0\n",
      "Reward: 60.0\n",
      "Timesteps: 322\n",
      "Game Frames Survived: 1619\n",
      "Epsilon: 0.9959445999998834\n",
      "Running Score (last 10 games): 112.5\n",
      "Running Reward (last 10 games): 112.5\n",
      "Explored: 6, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 7\n",
      "EXPLORATION\n",
      "Score: 20.0\n",
      "Reward: 20.0\n",
      "Timesteps: 328\n",
      "Game Frames Survived: 1643\n",
      "Epsilon: 0.9953541999998664\n",
      "Running Score (last 10 games): 99.28571428571429\n",
      "Running Reward (last 10 games): 99.28571428571429\n",
      "Explored: 7, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 8\n",
      "EXPLORATION\n",
      "Score: 55.0\n",
      "Reward: 55.0\n",
      "Timesteps: 293\n",
      "Game Frames Survived: 1491\n",
      "Epsilon: 0.9948267999998512\n",
      "Running Score (last 10 games): 93.75\n",
      "Running Reward (last 10 games): 93.75\n",
      "Explored: 8, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 9\n",
      "EXPLORATION\n",
      "Score: 80.0\n",
      "Reward: 80.0\n",
      "Timesteps: 279\n",
      "Game Frames Survived: 1423\n",
      "Epsilon: 0.9943245999998368\n",
      "Running Score (last 10 games): 92.22222222222223\n",
      "Running Reward (last 10 games): 92.22222222222223\n",
      "Explored: 9, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 10\n",
      "EXPLORATION\n",
      "Score: 110.0\n",
      "Reward: 110.0\n",
      "Timesteps: 356\n",
      "Game Frames Survived: 1805\n",
      "Epsilon: 0.9936837999998184\n",
      "Running Score (last 10 games): 94.0\n",
      "Running Reward (last 10 games): 94.0\n",
      "Explored: 10, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 11\n",
      "EXPLORATION\n",
      "Score: 95.0\n",
      "Reward: 95.0\n",
      "Timesteps: 309\n",
      "Game Frames Survived: 1569\n",
      "Epsilon: 0.9931275999998024\n",
      "Running Score (last 10 games): 81.0\n",
      "Running Reward (last 10 games): 81.0\n",
      "Explored: 11, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 12\n",
      "EXPLORATION\n",
      "Score: 120.0\n",
      "Reward: 120.0\n",
      "Timesteps: 333\n",
      "Game Frames Survived: 1683\n",
      "Epsilon: 0.9925281999997851\n",
      "Running Score (last 10 games): 84.5\n",
      "Running Reward (last 10 games): 84.5\n",
      "Explored: 12, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 13\n",
      "EXPLORATION\n",
      "Score: 55.0\n",
      "Reward: 55.0\n",
      "Timesteps: 382\n",
      "Game Frames Survived: 1921\n",
      "Epsilon: 0.9918405999997654\n",
      "Running Score (last 10 games): 83.5\n",
      "Running Reward (last 10 games): 83.5\n",
      "Explored: 13, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 14\n",
      "EXPLORATION\n",
      "Score: 110.0\n",
      "Reward: 110.0\n",
      "Timesteps: 395\n",
      "Game Frames Survived: 1977\n",
      "Epsilon: 0.9911295999997449\n",
      "Running Score (last 10 games): 82.5\n",
      "Running Reward (last 10 games): 82.5\n",
      "Explored: 14, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 15\n",
      "EXPLORATION\n",
      "Score: 355.0\n",
      "Reward: 355.0\n",
      "Timesteps: 542\n",
      "Game Frames Survived: 2737\n",
      "Epsilon: 0.9901539999997169\n",
      "Running Score (last 10 games): 106.0\n",
      "Running Reward (last 10 games): 106.0\n",
      "Explored: 15, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 16\n",
      "EXPLORATION\n",
      "Score: 120.0\n",
      "Reward: 120.0\n",
      "Timesteps: 436\n",
      "Game Frames Survived: 2187\n",
      "Epsilon: 0.9893691999996943\n",
      "Running Score (last 10 games): 112.0\n",
      "Running Reward (last 10 games): 112.0\n",
      "Explored: 16, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 17\n",
      "EXPLORATION\n",
      "Score: 215.0\n",
      "Reward: 215.0\n",
      "Timesteps: 481\n",
      "Game Frames Survived: 2419\n",
      "Epsilon: 0.9885033999996694\n",
      "Running Score (last 10 games): 131.5\n",
      "Running Reward (last 10 games): 131.5\n",
      "Explored: 17, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 18\n",
      "EXPLORATION\n",
      "Score: 45.0\n",
      "Reward: 45.0\n",
      "Timesteps: 251\n",
      "Game Frames Survived: 1257\n",
      "Epsilon: 0.9880515999996564\n",
      "Running Score (last 10 games): 130.5\n",
      "Running Reward (last 10 games): 130.5\n",
      "Explored: 18, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 19\n",
      "EXPLORATION\n",
      "Score: 75.0\n",
      "Reward: 75.0\n",
      "Timesteps: 244\n",
      "Game Frames Survived: 1221\n",
      "Epsilon: 0.9876123999996438\n",
      "Running Score (last 10 games): 130.0\n",
      "Running Reward (last 10 games): 130.0\n",
      "Explored: 19, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 20\n",
      "EXPLORATION\n",
      "Score: 75.0\n",
      "Reward: 75.0\n",
      "Timesteps: 352\n",
      "Game Frames Survived: 1781\n",
      "Epsilon: 0.9869787999996256\n",
      "Running Score (last 10 games): 126.5\n",
      "Running Reward (last 10 games): 126.5\n",
      "Explored: 20, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 21\n",
      "EXPLORATION\n",
      "Score: 105.0\n",
      "Reward: 105.0\n",
      "Timesteps: 365\n",
      "Game Frames Survived: 1839\n",
      "Epsilon: 0.9863217999996067\n",
      "Running Score (last 10 games): 127.5\n",
      "Running Reward (last 10 games): 127.5\n",
      "Explored: 21, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 22\n",
      "EXPLORATION\n",
      "Score: 125.0\n",
      "Reward: 125.0\n",
      "Timesteps: 376\n",
      "Game Frames Survived: 1905\n",
      "Epsilon: 0.9856449999995872\n",
      "Running Score (last 10 games): 128.0\n",
      "Running Reward (last 10 games): 128.0\n",
      "Explored: 22, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 23\n",
      "EXPLORATION\n",
      "Score: 105.0\n",
      "Reward: 105.0\n",
      "Timesteps: 385\n",
      "Game Frames Survived: 1935\n",
      "Epsilon: 0.9849519999995673\n",
      "Running Score (last 10 games): 133.0\n",
      "Running Reward (last 10 games): 133.0\n",
      "Explored: 23, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 24\n",
      "EXPLORATION\n",
      "Score: 30.0\n",
      "Reward: 30.0\n",
      "Timesteps: 214\n",
      "Game Frames Survived: 1095\n",
      "Epsilon: 0.9845667999995562\n",
      "Running Score (last 10 games): 125.0\n",
      "Running Reward (last 10 games): 125.0\n",
      "Explored: 24, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 25\n",
      "EXPLORATION\n",
      "Score: 85.0\n",
      "Reward: 85.0\n",
      "Timesteps: 263\n",
      "Game Frames Survived: 1337\n",
      "Epsilon: 0.9840933999995426\n",
      "Running Score (last 10 games): 98.0\n",
      "Running Reward (last 10 games): 98.0\n",
      "Explored: 25, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 26\n",
      "EXPLORATION\n",
      "Score: 90.0\n",
      "Reward: 90.0\n",
      "Timesteps: 295\n",
      "Game Frames Survived: 1495\n",
      "Epsilon: 0.9835623999995273\n",
      "Running Score (last 10 games): 95.0\n",
      "Running Reward (last 10 games): 95.0\n",
      "Explored: 26, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 27\n",
      "EXPLORATION\n",
      "Score: 210.0\n",
      "Reward: 210.0\n",
      "Timesteps: 407\n",
      "Game Frames Survived: 2035\n",
      "Epsilon: 0.9828297999995063\n",
      "Running Score (last 10 games): 94.5\n",
      "Running Reward (last 10 games): 94.5\n",
      "Explored: 27, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 28\n",
      "EXPLORATION\n",
      "Score: 155.0\n",
      "Reward: 155.0\n",
      "Timesteps: 351\n",
      "Game Frames Survived: 1753\n",
      "Epsilon: 0.9821979999994881\n",
      "Running Score (last 10 games): 105.5\n",
      "Running Reward (last 10 games): 105.5\n",
      "Explored: 28, Exploited: 0\n",
      "xxxxxxxxxx\n",
      "running reward: 105.50 at episode 28, frame count 10000\n",
      "----------\n",
      "----------\n",
      "Game Number: 29\n",
      "EXPLORATION\n",
      "Score: 125.0\n",
      "Reward: 125.0\n",
      "Timesteps: 427\n",
      "Game Frames Survived: 2141\n",
      "Epsilon: 0.981429399999466\n",
      "Running Score (last 10 games): 110.5\n",
      "Running Reward (last 10 games): 110.5\n",
      "Explored: 29, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 30\n",
      "EXPLORATION\n",
      "Score: 155.0\n",
      "Reward: 155.0\n",
      "Timesteps: 455\n",
      "Game Frames Survived: 2279\n",
      "Epsilon: 0.9806103999994424\n",
      "Running Score (last 10 games): 118.5\n",
      "Running Reward (last 10 games): 118.5\n",
      "Explored: 30, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 31\n",
      "EXPLORATION\n",
      "Score: 110.0\n",
      "Reward: 110.0\n",
      "Timesteps: 379\n",
      "Game Frames Survived: 1909\n",
      "Epsilon: 0.9799281999994228\n",
      "Running Score (last 10 games): 119.0\n",
      "Running Reward (last 10 games): 119.0\n",
      "Explored: 31, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 32\n",
      "EXPLORATION\n",
      "Score: 140.0\n",
      "Reward: 140.0\n",
      "Timesteps: 485\n",
      "Game Frames Survived: 2433\n",
      "Epsilon: 0.9790551999993977\n",
      "Running Score (last 10 games): 120.5\n",
      "Running Reward (last 10 games): 120.5\n",
      "Explored: 32, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 33\n",
      "EXPLORATION\n",
      "Score: 75.0\n",
      "Reward: 75.0\n",
      "Timesteps: 233\n",
      "Game Frames Survived: 1185\n",
      "Epsilon: 0.9786357999993857\n",
      "Running Score (last 10 games): 117.5\n",
      "Running Reward (last 10 games): 117.5\n",
      "Explored: 33, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 34\n",
      "EXPLORATION\n",
      "Score: 135.0\n",
      "Reward: 135.0\n",
      "Timesteps: 411\n",
      "Game Frames Survived: 2083\n",
      "Epsilon: 0.9778959999993644\n",
      "Running Score (last 10 games): 128.0\n",
      "Running Reward (last 10 games): 128.0\n",
      "Explored: 34, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 35\n",
      "EXPLORATION\n",
      "Score: 70.0\n",
      "Reward: 70.0\n",
      "Timesteps: 316\n",
      "Game Frames Survived: 1605\n",
      "Epsilon: 0.977327199999348\n",
      "Running Score (last 10 games): 126.5\n",
      "Running Reward (last 10 games): 126.5\n",
      "Explored: 35, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 36\n",
      "EXPLORATION\n",
      "Score: 115.0\n",
      "Reward: 115.0\n",
      "Timesteps: 411\n",
      "Game Frames Survived: 2057\n",
      "Epsilon: 0.9765873999993268\n",
      "Running Score (last 10 games): 129.0\n",
      "Running Reward (last 10 games): 129.0\n",
      "Explored: 36, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 37\n",
      "EXPLORATION\n",
      "Score: 110.0\n",
      "Reward: 110.0\n",
      "Timesteps: 227\n",
      "Game Frames Survived: 1155\n",
      "Epsilon: 0.976178799999315\n",
      "Running Score (last 10 games): 119.0\n",
      "Running Reward (last 10 games): 119.0\n",
      "Explored: 37, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 38\n",
      "EXPLORATION\n",
      "Score: 230.0\n",
      "Reward: 230.0\n",
      "Timesteps: 572\n",
      "Game Frames Survived: 2883\n",
      "Epsilon: 0.9751491999992854\n",
      "Running Score (last 10 games): 126.5\n",
      "Running Reward (last 10 games): 126.5\n",
      "Explored: 38, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 39\n",
      "EXPLORATION\n",
      "Score: 180.0\n",
      "Reward: 180.0\n",
      "Timesteps: 358\n",
      "Game Frames Survived: 1789\n",
      "Epsilon: 0.9745047999992669\n",
      "Running Score (last 10 games): 132.0\n",
      "Running Reward (last 10 games): 132.0\n",
      "Explored: 39, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 40\n",
      "EXPLORATION\n",
      "Score: 175.0\n",
      "Reward: 175.0\n",
      "Timesteps: 404\n",
      "Game Frames Survived: 2023\n",
      "Epsilon: 0.973777599999246\n",
      "Running Score (last 10 games): 134.0\n",
      "Running Reward (last 10 games): 134.0\n",
      "Explored: 40, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 41\n",
      "EXPLORATION\n",
      "Score: 80.0\n",
      "Reward: 80.0\n",
      "Timesteps: 299\n",
      "Game Frames Survived: 1511\n",
      "Epsilon: 0.9732393999992305\n",
      "Running Score (last 10 games): 131.0\n",
      "Running Reward (last 10 games): 131.0\n",
      "Explored: 41, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 42\n",
      "EXPLORATION\n",
      "Score: 35.0\n",
      "Reward: 35.0\n",
      "Timesteps: 372\n",
      "Game Frames Survived: 1881\n",
      "Epsilon: 0.9725697999992112\n",
      "Running Score (last 10 games): 120.5\n",
      "Running Reward (last 10 games): 120.5\n",
      "Explored: 42, Exploited: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/wrappers/frame_stack.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mStacked\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/wrappers/atari_preprocessing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;34m\"TimeLimit.truncated\"\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \"\"\"\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpassive_env_step_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/envs/atari/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action_ind)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Instantiate environment\n",
    "env = instantiate_environmnent()\n",
    "\n",
    "model = DuelingDQN(6)\n",
    "model_target = DuelingDQN(6)\n",
    "\n",
    "# Configuration paramaters for the whole setup\n",
    "gamma = 0.99 # Discount factor for past rewards\n",
    "epsilon = 1.0 # Epsilon greedy parameter\n",
    "epsilon_min = 0.1 # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0 # Maximum epsilon greedy parameter\n",
    "epsilon_interval = epsilon_max - epsilon_min # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 32 # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "\n",
    "# Optimizer improves training time\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "\n",
    "# Replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "score_history = []\n",
    "\n",
    "# Information variables\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "explored = 0\n",
    "exploited = 0\n",
    "\n",
    "# Number of frames to take random action and observe output and greediness factor\n",
    "epsilon_random_frames = 150000 # Should change depending on training time\n",
    "epsilon_greedy_frames = 500000 # Should change depending on training time\n",
    "\n",
    "# Maximum replay length\n",
    "max_memory_length = 100000\n",
    "\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "\n",
    "while True: # Run until solved\n",
    "    state = np.asarray(env.reset()).reshape(84, 84, 4)\n",
    "    \n",
    "    # Episode information\n",
    "    frames_this_episode = 0\n",
    "    episode_reward = 0\n",
    "    score = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "        frames_this_episode += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values from environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            \n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.asarray(state_next).reshape(84, 84, 4)\n",
    "        episode_frame_number = _[\"episode_frame_number\"]\n",
    "        score += reward\n",
    "        \n",
    "        # Reward modifier (This will also affect score)\n",
    "        reward = reward_function(reward)\n",
    "        \n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            print(\"xxxxxxxxxx\")\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            print(\"----------\")\n",
    "            print(\"----------\")\n",
    "            print(f\"Game Number: {episode_count + 1}\")\n",
    "            if frame_count < epsilon_random_frames:\n",
    "                print(\"EXPLORATION\")\n",
    "                explored += 1\n",
    "            else:\n",
    "                print(\"EXPLOITATION\")\n",
    "                exploited += 1\n",
    "            print(f\"Score: {score}\")\n",
    "            print(f\"Reward: {episode_reward}\")\n",
    "            print(f\"Timesteps: {frames_this_episode}\")\n",
    "            print(f\"Game Frames Survived: {episode_frame_number}\")\n",
    "            print(f\"Epsilon: {epsilon}\")\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    score_history.append(score)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history[-10:])\n",
    "    running_score = np.mean(score_history[-10:])\n",
    "    \n",
    "    print(f\"Running Score (last 10 games): {running_score}\")\n",
    "    print(f\"Running Reward (last 10 games): {running_reward}\")\n",
    "    print(f\"Explored: {explored}, Exploited: {exploited}\")  \n",
    "    \n",
    "    # Save model checkpoint\n",
    "    if episode_count % 100 == 0:\n",
    "        model.save(\"model3\")\n",
    "    \n",
    "    episode_count += 1\n",
    "\n",
    "    if running_score > 1000:  # Condition to consider the task solved\n",
    "        print(\"xxxxxxxxxx\")\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "529a1aa7-6a37-40c3-8640-13d65940a305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Episode:1 Score:185.0\n",
      "Episode:2 Score:490.0\n",
      "Episode:3 Score:185.0\n",
      "Episode:4 Score:240.0\n",
      "Episode:5 Score:185.0\n",
      "Episode:6 Score:185.0\n",
      "Episode:7 Score:185.0\n",
      "Episode:8 Score:215.0\n",
      "Episode:9 Score:185.0\n",
      "Episode:10 Score:215.0\n"
     ]
    }
   ],
   "source": [
    "env = instantiate_environmnent()\n",
    "\n",
    "model = keras.models.load_model(\"model2\")\n",
    "video = VideoRecorder(env, \"model.mp4\", enabled=True)\n",
    "\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = np.asarray(env.reset()).reshape(84, 84, 4)\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        batch_state = tf.expand_dims(state, 0)\n",
    "        action = np.argmax(model.predict(batch_state)[0])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        state = np.asarray(state).reshape(84, 84, 4)\n",
    "        video.capture_frame()\n",
    "        score += reward\n",
    "    \n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "\n",
    "video.close()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a62d3e4f09b941b4f258c514afa01f6b97bf0963addfe799c5a959f2b236a57c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
