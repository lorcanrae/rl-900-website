{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67381f35-7f4c-40e6-a37f-04cbaa8f59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Setup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2bd055-1b32-425e-8473-0ad4b03fdc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 22:59:49.508028: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lrae/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-08-07 22:59:49.508068: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import gym\n",
    "from gym.wrappers.atari_preprocessing import AtariPreprocessing\n",
    "from gym.wrappers.frame_stack import FrameStack\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae7e957b-8932-4855-b1cd-0f41f068086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN,self).__init__()\n",
    "        \n",
    "        self.conv1 = keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=8,\n",
    "            activation='relu',\n",
    "            input_shape=(84, 84, 4)\n",
    "        )\n",
    "        self.pool1 = keras.layers.MaxPool2D(pool_size=(4, 4))\n",
    "        self.conv2 = keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=4,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.pool2 = keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        self.conv3 = keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=3,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.dense1 = keras.layers.Dense(\n",
    "            units=512,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.action = keras.layers.Dense(units=num_actions, activation='linear')\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, states):\n",
    "        states = tf.cast(states, tf.float32)\n",
    "        x = self.conv1(states)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        return self.action(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99e4b553-d7ab-438d-aeab-1334c1348f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        \n",
    "        self.conv1 = keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=8,\n",
    "            strides=4,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=keras.initializers.VarianceScaling(2.0),\n",
    "            data_format=\"channels_last\",\n",
    "            input_shape=(84,84,4)\n",
    "        )\n",
    "        self.conv2 = keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=4,\n",
    "            strides=2,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=keras.initializers.VarianceScaling(2.0),\n",
    "        )\n",
    "        self.conv3 = keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=keras.initializers.VarianceScaling(2.0),\n",
    "        )\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.dense1 = keras.layers.Dense(\n",
    "            units=512,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=keras.initializers.VarianceScaling(2.0),\n",
    "        )\n",
    "        self.V = keras.layers.Dense(1)\n",
    "        self.A = keras.layers.Dense(num_actions)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, states):\n",
    "        states = tf.cast(states, tf.float32)\n",
    "        x = self.conv1(states)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        Q = V + tf.subtract(A, tf.reduce_mean(A, axis=1, keepdims=True))\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0422886e-9d9c-4e28-b5de-9700e721279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(reward):\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "603b6d38-e28d-4a79-a7cb-e588c125ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environmnent and preprocess\n",
    "def instantiate_environmnent():\n",
    "    env = gym.make(\"SpaceInvadersNoFrameskip-v4\")\n",
    "    env = AtariPreprocessing(env, grayscale_newaxis=False, frame_skip=5)\n",
    "    env = FrameStack(env, 4)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f72388b-9cd3-497a-a40e-80e012126530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "----------\n",
      "Game Number: 1\n",
      "EXPLORATION\n",
      "Score: 335.0\n",
      "Reward: 335.0\n",
      "Timesteps: 680\n",
      "Game Frames Survived: 3407\n",
      "Epsilon: 0.9987759999999648\n",
      "Running Score (last 10 games): 335.0\n",
      "Running Reward (last 10 games): 335.0\n",
      "Explored: 1, Exploited: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 23:01:13.354973: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model3/assets\n",
      "----------\n",
      "----------\n",
      "Game Number: 2\n",
      "EXPLORATION\n",
      "Score: 130.0\n",
      "Reward: 130.0\n",
      "Timesteps: 400\n",
      "Game Frames Survived: 2005\n",
      "Epsilon: 0.9980559999999441\n",
      "Running Score (last 10 games): 232.5\n",
      "Running Reward (last 10 games): 232.5\n",
      "Explored: 2, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 3\n",
      "EXPLORATION\n",
      "Score: 100.0\n",
      "Reward: 100.0\n",
      "Timesteps: 235\n",
      "Game Frames Survived: 1199\n",
      "Epsilon: 0.9976329999999319\n",
      "Running Score (last 10 games): 188.33333333333334\n",
      "Running Reward (last 10 games): 188.33333333333334\n",
      "Explored: 3, Exploited: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1082\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1085\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backward_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbackward_function_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m       return backward._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   1307\u001b[0m           processed_args, remapped_captures)\n\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "NUM_ACTIONS = 6\n",
    "\n",
    "# Instantiate environment\n",
    "env = instantiate_environmnent()\n",
    "\n",
    "model = DQN(NUM_ACTIONS)\n",
    "model_target = DQN(NUM_ACTIONS)\n",
    "\n",
    "# Configuration paramaters for the whole setup\n",
    "gamma = 0.99 # Discount factor for past rewards\n",
    "epsilon = 1.0 # Epsilon greedy parameter\n",
    "epsilon_min = 0.1 # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0 # Maximum epsilon greedy parameter\n",
    "epsilon_interval = epsilon_max - epsilon_min # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 32 # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "# Optimizer improves training time\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "score_history = []\n",
    "\n",
    "# Information variables\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "explored = 0\n",
    "exploited = 0\n",
    "\n",
    "# Number of frames to take random action and observe output and greediness factor\n",
    "epsilon_random_frames = 150000 # Should change depending on training time\n",
    "epsilon_greedy_frames = 500000 # Should change depending on training time\n",
    "\n",
    "# Maximum replay length\n",
    "max_memory_length = 100000\n",
    "\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "\n",
    "while True: # Run until solved\n",
    "    state = np.asarray(env.reset()).reshape(84, 84, 4)\n",
    "    \n",
    "    # Episode information\n",
    "    frames_this_episode = 0\n",
    "    episode_reward = 0\n",
    "    score = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "        frames_this_episode += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values from environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            \n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.asarray(state_next).reshape(84, 84, 4)\n",
    "        episode_frame_number = _[\"episode_frame_number\"]\n",
    "        score += reward\n",
    "        \n",
    "        # Reward modifier (This will also affect score)\n",
    "        reward = reward_function(reward)\n",
    "        \n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            print(\"xxxxxxxxxx\")\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            print(\"----------\")\n",
    "            print(\"----------\")\n",
    "            print(f\"Game Number: {episode_count + 1}\")\n",
    "            if frame_count < epsilon_random_frames:\n",
    "                print(\"EXPLORATION\")\n",
    "                explored += 1\n",
    "            else:\n",
    "                print(\"EXPLOITATION\")\n",
    "                exploited += 1\n",
    "            print(f\"Score: {score}\")\n",
    "            print(f\"Reward: {episode_reward}\")\n",
    "            print(f\"Timesteps: {frames_this_episode}\")\n",
    "            print(f\"Game Frames Survived: {episode_frame_number}\")\n",
    "            print(f\"Epsilon: {epsilon}\")\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    score_history.append(score)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history[-10:])\n",
    "    running_score = np.mean(score_history[-10:])\n",
    "    \n",
    "    print(f\"Running Score (last 10 games): {running_score}\")\n",
    "    print(f\"Running Reward (last 10 games): {running_reward}\")\n",
    "    print(f\"Explored: {explored}, Exploited: {exploited}\")  \n",
    "    \n",
    "    # Save model checkpoint\n",
    "    if episode_count % 100 == 0:\n",
    "        model.save(\"model3\")\n",
    "    \n",
    "    episode_count += 1\n",
    "\n",
    "    if running_score > 1000:  # Condition to consider the task solved\n",
    "        print(\"xxxxxxxxxx\")\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "529a1aa7-6a37-40c3-8640-13d65940a305",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at model2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32422/4026887171.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstantiate_environmnent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoRecorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.mp4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             raise ImportError(\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at model2"
     ]
    }
   ],
   "source": [
    "env = instantiate_environmnent()\n",
    "\n",
    "model = keras.models.load_model(\"model2\")\n",
    "video = VideoRecorder(env, \"model.mp4\", enabled=True)\n",
    "\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = np.asarray(env.reset()).reshape(84, 84, 4)\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        batch_state = tf.expand_dims(state, 0)\n",
    "        action = np.argmax(model.predict(batch_state)[0])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        state = np.asarray(state).reshape(84, 84, 4)\n",
    "        video.capture_frame()\n",
    "        score += reward\n",
    "    \n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "\n",
    "video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be48ca-e2ca-4a9c-a9c3-3980dbf19671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a62d3e4f09b941b4f258c514afa01f6b97bf0963addfe799c5a959f2b236a57c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
